import re
import nltk
import numpy as np
from nltk.corpus import stopwords
from gensim.models import Word2Vec
from sklearn.preprocessing import Normalizer
from sklearn.decomposition import PCA
from sklearn.decomposition import NMF
from sklearn.decomposition import TruncatedSVD
nltk.download('stopwords')
nltk.download('punkt')


class TweetVectorizer():
    """

    A simple Vectorizer object that converts a list of tweets into vectors
    equal to the sum of the word2vec vector for each word in the tweet
    """
    def __init__(self, lowercase=True, max_df=float("inf"),
                 min_df=1, max_features=None, stop_words=frozenset([])):

        self.lowercase = lowercase
        self.max_df = max_df
        self.min_df = min_df
        self.max_features = max_features
        self.stop_words = stop_words
        self.word2vec = None

    def __repr__(self):
        pass

    def get_feature_names(self):
        """
        Returns
        -------
        vocabulary?
        """
        vocabulary = []
        if self.word2vec is not None:
            vocabulary = self.word2vec.wv.vocab
        return vocabulary

    def get_model(self):
        """
        Returns
        -------
        word2vec: word2vec object
            the word2vec model generated by the input dataset
        """
        word2vec = self.word2vec
        return word2vec

    def _check_params(self):
        pass

    def words_matrix(self):
        """
        Returns
        -------
        word_vectors: list of arrays
            the word vector of each word in the word2vec vocabulary
        """
        word2vec = self.word2vec

        word_vectors = []

        # check if model has been fitted
        if word2vec is not None:
            vocabulary = word2vec.wv.vocab
            for word in vocabulary:

                # add the word vector to the list of vectors
                word_vectors += [word2vec.wv.get_vector(word)]

        return word_vectors

    def _tokenize(self, dataset):
        """

        process and tokenize the tweets

        Parameters
        ----------
        dataset: list
            a list of unicode strings representing tweets

        Returns
        -------
        tokenized_tweets: list
            a list of lists of strings
        """

        tokenized_tweets = []
        for tweet in dataset:

            # process the tweet
            processed_tweet = tweet
            if self.lowercase:
                processed_tweet = processed_tweet.lower()

            # I don't know what these two lines do
            processed_tweet = re.sub('[^a-zA-Z]', ' ', processed_tweet)
            processed_tweet = re.sub(r'\s+', ' ', processed_tweet)

            # split sentence into list of words and remove stopwords
            tokens = nltk.word_tokenize(processed_tweet)
            tokenized_tweets += [[token for token in tokens if token not in self.stop_words]]

        return tokenized_tweets

    def _vectorize(self, tokenized_tweets, model):
        """

        create a word2vec model using the tokenized tweets
        then for each tweet, find the sum of all the vectors of all its words

        Parameters
        ----------
        tokenized_tweets: list
            a list of unicode strings representing tweets

        Returns
        -------
        tweet_vectors: list
            a list of arrays of real numbers representing tweets
        """

        word2vec = model
        vector_size = model.vector_size

        tweet_vectors = []
        for tweet in tokenized_tweets:

            # get the vector of the sum of the word2vec vectors of each of word in the tweet
            tweet_vector = np.zeros(vector_size)
            for word in tweet:
                if word in word2vec.wv.vocab:
                    tweet_vector = np.add(tweet_vector, word2vec.wv.get_vector(word))

            tweet_vectors += [tweet_vector]

        return tweet_vectors

    def fit(self, dataset, y=None):
        """
        fit a word2vec model onto the dataset

        Parameters
        ----------
        dataset:
            a list of tweet text strings
        """
        tokenized_tweets = self._tokenize(dataset)
        word2vec = Word2Vec(tokenized_tweets, min_count=1)
        self.word2vec = word2vec
        return self

    def transform(self, dataset):
        """
        use the fitted word2vec model fitted onto the
        tweets to generate a vector for each tweet equal
        to the sum of all the word vectors in the tweet

        Parameters
        ----------
        dataset: list
            a list of tweets as strings

        Return
        ------
        tweet_vectors: list
            a list of vectors

        """

        # check if it's fitted
        word2vec = self.word2vec
        if word2vec is not None:
            tokenized_tweets = self._tokenize(dataset)
            tweet_vectors = self._vectorize(tokenized_tweets, word2vec)
            return tweet_vectors

    def fit_transform(self, dataset):
        """
        call self.fit() then self.transform()

        Parameters
        ----------
        same as self.transform

        Return
        ------
        same as self.transform
        """
        tokenized_tweets = self._tokenize(dataset)
        word2vec = Word2Vec(tokenized_tweets, min_count=1)
        self.word2vec = word2vec
        word_vectors = self._vectorize(tokenized_tweets, word2vec)
        return word_vectors

    def summary(self):
        pass


def topic_classification(tweets, n_topics=1):
    """
    generate word2vec model and use it to represent each tweet as
    a vector. Then use PCA to identify topics in the tweets and print
    the top words that are associated with that topic

    Parameters
    ----------
    tweets: list
        a list of unicode strings representing tweets
    n_topics: Integer
        an integer greater than 0 representing the number of topics
    """

    print("transforming tweets into vectors...")
    stop = frozenset(stopwords.words('english'))
    vectorizer = TweetVectorizer(stop_words=stop)
    tweet_vectors = vectorizer.fit_transform(tweets)
    word2vec = vectorizer.get_model()

    print("Fitting the PCA model..")
    normalizer = Normalizer()
    pca = PCA(n_components=n_topics)
    pca.fit_transform(normalizer.fit_transform(tweet_vectors))
    pca.fit_transform(tweet_vectors)

    for topic_idx, topic in enumerate(pca.components_):
        print("*" * 200)
        print("Topic #%d:" % topic_idx)
        print(word2vec.wv.similar_by_vector(topic))
        print(" ")


def topics(tweets, n_topics):
    """
    generate word2vec model from the tweets and then generate
    a matrix where each column is a word2vec vector of a word
    in the tweet vocabulary. Then use PCA to identify topics in
    the tweets and printthe top words that are associated with that topic

    Parameters
    ----------
    tweets: list
        a list of unicode strings representing tweets
    n_topics: Integer
        an integer greater than 0 representing the number of topics
    """
    print("transforming tweets into vectors...")
    stop = frozenset(stopwords.words('english'))
    vectorizer = TweetVectorizer(stop_words=stop).fit(tweets)
    tweet_vectors = vectorizer.words_matrix()
    word2vec = vectorizer.get_model()

    print("Fitting the PCA model..")
    normalizer = Normalizer()
    pca = PCA(n_components=n_topics)
    pca.fit_transform(normalizer.fit_transform(tweet_vectors))
    for topic_idx, topic in enumerate(pca.components_):
        print("*" * 200)
        print("Topic #%d:" % topic_idx)
        print(word2vec.wv.similar_by_vector(topic))
        print(" ")


if __name__ == "__main__":

    print(" ")

    # get unique tweets


    tweets = [
        "the house had a tiny little mouse",
        "the cat saw the mouse",
        "the mouse ran away from the house",
        "the cat finally ate the mouse",
        "the end of the mouse story"
    ]
    topics(tweets, n_topics=20)

    # test cases
    #
    # stop = frozenset(stopwords.words('english'))
    # vectorizer = TweetVectorizer(stop_words=stop)
    # tweet_vectors1 = vectorizer.fit_transform(tweets)
    # tweet_vectors2 = vectorizer.fit(tweets).transform(tweets)
    # print(tweet_vectors1)
    # print(tweet_vectors2)
    # for i in range(len(tweets)):
    #     print(tweet_vectors1[i] == tweet_vectors2[i])